Apps Need A New Data Center Stack, Because Infra is Difficult to Get Right: Google, Facebook and Amazon serve billions of users overall   and many, many millions concurrently   and store incredible amounts of data. Yet they rarely crash. Once it finally decided to make a significant investment in infrastructure engineering, Twitter all but slayed the fail whale. Among some of the better-known technologies these companies have produced are MapReduce, Hadoop, Cassandra and Kafka. A new suite of tools   some created by startups, some in labs, some as open source projects   has also emerged in their image, designed to make applications perform and scale better, and sometimes to enable new capabilities altogether. These include technologies such as Spark, Storm and Elasticsearch. In concert with these advances, new architectures also caught on in order to address the problems that come with trying to develop applications that can run reliably at such extreme scale. One is the concept of microservices, which involves treating applications as a collection of services that might serve multiple applications, rather than as monolithic entities with their own dedicated components. Among other things, a service-oriented approach results in less dependency among components and the ability to scale individual services without re-architecting the entire application. Another big architectural trend has been containerization, whether it s done via developer-friendly means like Docker or lower-level means like Linux control groups. Containers can make it easier to plug applications into distributed services and to shift the focus from deciding where something should run; instead, containers let developers focus on what their applications need to run. Taken as a whole, this new collection of distributed services and architectural techniques could be called the  data center application stack.  Anyone building an application that serves millions of users on multiple platforms, and can make use of the volume, variety and velocity of data today, is going to be using this collection of services or something very much like it. In fact, these technologies are all gaining popularity fast. Many are already staples in the technology repertoires of startups trying to deliver everything from the next huge consumer app to the next Salesforce.com.  Big data,   real-time  and  Internet of Things  are more than just buzzwords. They re imperatives for corporate success in many parts of the 21st century economy. However, the elephant in the room   which you might not hear from IT vendors, open source advocates or expert Facebook engineers   is that building out these capabilities is hard. Deploy, manage and scale Hadoop. Deploy, manage and scale Cassandra. Deploy, manage and scale Kubernetes. Rinse and repeat for every framework or service you want to use. At some point, companies probably will want to give a little thought to actually writing the application, building the data pipeline and making sure the architecture is resilient. Huge, engineer-rich companies such as Google and Microsoft solved (or largely solved) this problem for themselves with systems like Borg and Autopilot, respectively. The systems automatically manage resource allocation and high availability for the services and applications that run across their millions of servers. Algorithms, not developers or software architects, determine where things run and on how many machines. Sure, they re great systems, but they re also proprietary. Google only recently officially acknowledged Borg s existence by publishing a paper on it. Microsoft has done very little public discussion of Autopilot. Neither are for sale.