Algorithms to find online trolls - trolls focus on a small number of stories and attract the most responses: Scientists have figured out how to tell when someone is an online troll: Spotting an online troll is pretty easy for your average Internet user: They're the jerk hijacking otherwise earnest online conversations for their own amusement, often with the help of straw man arguments and profanities. And a lot of online moderation schemes today rely on a large-scale version of this individual model: There are literally people whose job is reviewing posts that users marked as abusive or otherwise in violation of a site's commenting guidelines. But there could be a better way. What if there was software that could predict if a user was going to be a troll before their behavior could tear online communities apart  That's one of the questions that a study submitted this month to the 9th International Conference on Web and Social Media by researchers at Stanford and Cornell universities hopes to answer. The researchers --  Justin Cheng, Cristian Danescu-Niculescu-Mizil and Jure Leskovec -- waded through 18 months of user activity in the comment sections of CNN.com, conservative political news site Breitbart.com, and gaming site IGN.com looking for antisocial activity. Using data provided by commenting platform Disqus, they were eventually able to identify what they called "future banned users" -- commenters who were later blocked from the site for bad behavior. Those users, they found, tended to focus their comments on a small number of stories and were more likely to post things otherwise irrelevant to the overall conversation. Trolls' behavior also tended to get worse over time, according to the researchers -- and they were generally successful at getting a rise out of those in an online community. "They receive more replies than average users, suggesting that they might be successful in luring others into fruitless, time-consuming discussions," the researchers said. Some platforms are looking at ways to limit the impact of disruptive users without actually banning them. Twitter, for instance, said this week that it is testing out a product that flags potentially abusive tweets based on a "wide range of signals," such as account age and similarities to previous messages that staff deemed abusive, and then limits their reach. But regardless of how sites approach it, online harassment is a serious problem: Some 40 percent of adult Internet users have experienced it, according to a Pew Research Center study last year. And controlling the bad actors responsible behind some of the most aggressive behavior is a struggle for many online sites, including Facebook and Twitter.