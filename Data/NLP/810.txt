Microsoft Apologizes After Twitter Chat Bot Experiment Goes Awry: Microsoft apologized after Twitter users exploited its artificial-intelligence chat bot Tay, teaching it to spew racist, sexist and offensive remarks in what the company called a  coordinated attack  that took advantage of a  critical oversight.  The company will bring Tay back online once it s confident it can better anticipate malicious activities, he said.  A coordinated attack by a subset of people exploited a vulnerability in Tay. Although we had prepared for many types of abuses of the system, we had made a critical oversight for this specific attack,  Lee said, without elaborating. The company introduced Tay Wednesday to chat with humans on Twitter and other messaging platforms. The bot learns by parroting comments and then generating its own answers and statements based on all of its interactions. It was supposed to emulate the casual speech of a stereotypical millennial. Some users quickly tried to see how far they could push Tay.   In less than a day, Twitter s denizens realized Tay didn t really know what it was talking about and that it was easy to get the bot to make inappropriate comments on any taboo subject. People got Tay to deny the Holocaust, call for genocide and lynching, equate feminism to cancer and stump for Adolf Hitler. The worst tweets quickly disappeared from Twitter, and Tay itself also went offline  to absorb it all.  Some Twitter users appeared to think that Microsoft had also manually banned people from interacting with the bot. Others are asking why the company didn t build filters to prevent Tay from discussing certain topics, such as the Holocaust. The bot was targeted at 18- to 24-year-olds in the U.S. and meant to entertain and engage people through casual and playful conversation, according to Microsoft swebsite. Tay was built with public data and content from improvisational comedians. It s supposed to improve with more interactions, so should be able to better understand context and nuances over time. The bot s developers at Microsoft also collect the nickname, gender, favorite food, zip code and relationship status of anyone who chats with Tay.